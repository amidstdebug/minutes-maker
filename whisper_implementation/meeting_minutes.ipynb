{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sample Minutes Structure\n",
    "\n",
    "## 1. Opening Details\n",
    "This section sets the groundwork for the document, providing a snapshot of the meeting’s logistical details. It includes:\n",
    "\n",
    "- **Meeting Title:** The official name or subject of the meeting.\n",
    "- **Date and Time:** When the meeting took place.\n",
    "- **Location:** Where the meeting was held, including virtual meeting links if applicable.\n",
    "- **Attendees:** A list of individuals present at the meeting, including their titles or roles. Distinguish between members, guests, and absentees.\n",
    "- **Chairperson:** The individual presiding over the meeting.\n",
    "- **Secretary:** The person responsible for taking minutes.\n",
    "\n",
    "## 2. Agenda Items\n",
    "Following the opening details, this section outlines all the items discussed during the meeting. Each agenda item should be presented as a separate sub-section that includes:\n",
    "\n",
    "- **Item Title:** A brief title describing the agenda item.\n",
    "- **Presenter:** The name of the person who presented the item.\n",
    "- **Discussion Summary:** A concise summary of the discussion points, capturing the essence of what was talked about without delving into excessive detail.\n",
    "- **Action Items:** Specific actions to be taken, who is responsible for them, and any deadlines.\n",
    "\n",
    "## 3. Decisions Made\n",
    "This critical section documents the decisions reached during the meeting. For each decision, include:\n",
    "\n",
    "- **Decision Title:** A short, descriptive title of the decision.\n",
    "- **Description:** A brief explanation of the decision made.\n",
    "- **Responsible Party:** The person(s) or department(s) responsible for implementing the decision.\n",
    "- **Deadline:** If applicable, the timeline for implementation or review.\n",
    "\n",
    "## 4. Action Items\n",
    "Building on the decisions made, this section lists all the action items identified during the meeting, including those mentioned under agenda items but providing more detail. Each action item should specify:\n",
    "\n",
    "- **Action to be Taken:** A clear description of the task.\n",
    "- **Assigned To:** The individual(s) responsible for completing the action.\n",
    "- **Deadline:** The date by which the action should be completed.\n",
    "- **Status:** Initial status (typically \"Assigned\" or \"In Progress\").\n",
    "\n",
    "## 5. Closing Summary\n",
    "The final section wraps up the meeting minutes, providing a succinct summary of the meeting's outcomes and highlighting any next steps. This section includes:\n",
    "\n",
    "- **a. Meeting Adjournment:** The time the meeting concluded.\n",
    "- **b. Next Meeting:** Date, time, and location of the next scheduled meeting, if known.\n",
    "- **c. Closing Remarks:** Any final thoughts or comments from the chairperson, emphasizing the importance of the decisions made and the next steps.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a11f57399170090d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/justin/.local/lib/python3.10/site-packages (1.12.0)\r\n",
      "Requirement already satisfied: python-dotenv in /Users/justin/.local/lib/python3.10/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from openai) (4.3.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/justin/.local/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from openai) (0.27.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/justin/.local/lib/python3.10/site-packages (from openai) (2.6.2)\r\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/justin/.local/lib/python3.10/site-packages (from openai) (4.66.2)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from openai) (4.10.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/justin/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/justin/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai python-dotenv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:49:06.245296Z",
     "start_time": "2024-03-13T07:49:05.105273Z"
    }
   },
   "id": "1fa373363d56fb48",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import dotenv\n",
    "from openai import AzureOpenAI\n",
    "DEPLOYMENT = dotenv.get_key(dotenv.find_dotenv(), \"DEPLOYMENT\")\n",
    "ENDPOINT = dotenv.get_key(dotenv.find_dotenv(), \"ENDPOINT\")\n",
    "KEY = dotenv.get_key(dotenv.find_dotenv(), \"KEY\")\n",
    "VERSION = dotenv.get_key(dotenv.find_dotenv(), \"VERSION\")\n",
    "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
    "client = AzureOpenAI(\n",
    "\t# https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "\tapi_version=VERSION,\n",
    "\t# https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "\tazure_endpoint=ENDPOINT,\n",
    "\tapi_key=KEY\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:49:06.485558Z",
     "start_time": "2024-03-13T07:49:06.246557Z"
    }
   },
   "id": "232f608d30239f2e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# System prompts for each agent\n",
    "\n",
    "section_1 = \\\n",
    "\"\"\"\n",
    "Instructions for Writing Section 1: Opening Details with Detailed Information\n",
    "\n",
    "1. **Document Title and Header**: Start your document by labeling it as \"Meeting Minutes\" followed by the specific name of the meeting. This title should be bold and centered at the top of the page. For example:\n",
    "- Meeting Minutes: Quarterly Budget Review Meeting\n",
    "\n",
    "\n",
    "2. **Meeting Title**:\n",
    "- Below the document title, write \"Meeting Title:\" and then specify the exact purpose of the meeting. Choose a title that reflects the main objective or the broad area of discussion. This helps in identifying the focus of the meeting at a glance.\n",
    "\n",
    "3. **Date and Time**:\n",
    "- Next line, label \"Date and Time:\". Here, you will provide when the meeting occurred. Use the format \"Month Day, Year, from HH:MM AM/PM to HH:MM AM/PM.\" For virtual meetings across different time zones, specify the primary time zone and consider noting a few others for reference.\n",
    "- Example: \"March 12, 2024, from 10:00 AM to 12:00 PM EST (GMT-5)\"\n",
    "\n",
    "4. **Location**:\n",
    "- On the following line, write \"Location:\". Indicate the physical location with a full address if it's an in-person meeting. For virtual meetings, state the platform used (e.g., Zoom, Microsoft Teams) and include the link or meeting ID. It's helpful to mention whether it's a virtual or hybrid meeting.\n",
    "- Example for a physical meeting: \"City Hall, Room 101, 123 Main St., Springfield\"\n",
    "- Example for a virtual meeting: \"Virtual - Zoom Meeting, ID: 123-456-789, Link: [Zoom Meeting Link]\"\n",
    "\n",
    "5. **Attendees**:\n",
    "- Introduce a new section titled \"Attendees\". Divide this section into three parts: \"Members Present,\" \"Guests,\" and \"Absentees.\"\n",
    "- For each person listed, include their full name, title, or role within the organization, and their department if applicable. This ensures clarity on who was involved and their capacity.\n",
    "- Example:\n",
    "  ```\n",
    "  Members Present: John Doe, Finance Director; Jane Smith, Budget Analyst\n",
    "  Guests: Alex Johnson, External Auditor\n",
    "  Absentees: Mike Ross, Assistant Finance Director\n",
    "  ```\n",
    "\n",
    "6. **Chairperson and Secretary**:\n",
    "- Conclude this section by identifying the Chairperson and the Secretary of the meeting. Write \"Chairperson:\" followed by the name and title of the individual who led the meeting. Then, write \"Secretary:\" followed by the name and title of the person responsible for recording the minutes.\n",
    "- These roles are crucial for accountability and reference, as the Chairperson guides the meeting's flow, and the Secretary ensures all discussions and decisions are accurately documented.\n",
    "\n",
    "7. **Formatting Tips**:\n",
    "- Use bullet points or a numbered list for the Attendees section to enhance readability.\n",
    "- Maintain consistency in font and formatting throughout the document. Use a clear, professional font like Times New Roman or Arial, size 12.\n",
    "- Ensure all names and titles are accurately spelled, and double-check the date, time, and location for correctness.\n",
    "\n",
    "By following these detailed instructions, your Opening Details section will provide a comprehensive and clear overview of the foundational aspects of the meeting. This meticulous approach ensures that anyone reading the minutes can immediately understand the essential logistics of the meeting, setting a professional tone for the document.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "section_2 = \\\n",
    "\"\"\"\n",
    "Instructions for Writing Section 2: Agenda Items with Detailed Information\n",
    "\n",
    "1. **Section Header**:\n",
    "   - Start this section with a header titled \"Agenda Items.\" This indicates you're moving into the core content of the meeting, focusing on the topics discussed.\n",
    "\n",
    "2. **Itemizing Each Agenda Item**:\n",
    "   - List each agenda item as discussed during the meeting. Use a new sub-header for each item, which can be numbered or bulleted, depending on your preference. The title of each agenda item should be concise but descriptive enough to understand the topic at a glance.\n",
    "   - Example:\n",
    "     ```\n",
    "     1. Budget Overview for Q2\n",
    "     ```\n",
    "\n",
    "3. **Presenter**:\n",
    "   - For each agenda item, clearly identify the presenter or lead discussant. Write \"Presenter:\" followed by the individual's name and title. This assigns responsibility and provides a point of contact for follow-up questions.\n",
    "   - Example:\n",
    "     ```\n",
    "     Presenter: Jane Doe, Chief Financial Officer\n",
    "     ```\n",
    "\n",
    "4. **Discussion Summary**:\n",
    "   - Beneath the presenter, provide a summary of the discussion for that agenda item. Start with \"Discussion Summary:\" and then bullet or paragraph your summary. This should capture the key points discussed, significant viewpoints expressed, and any rationale behind decisions or opinions.\n",
    "   - Keep it concise but informative. Avoid unnecessary detail but ensure that someone who wasn't at the meeting can grasp what was discussed.\n",
    "   - Example:\n",
    "     ```\n",
    "     Discussion Summary:\n",
    "     - Reviewed the Q2 budget projections and identified potential overspending areas.\n",
    "     - Discussed reallocating funds from underutilized programs to areas of higher need.\n",
    "     ```\n",
    "\n",
    "5. **Action Items**:\n",
    "   - If any action items arise from the discussion, list these under the sub-heading \"Action Items:\". For each action, specify the task, who is responsible (assignee), and the deadline.\n",
    "   - Action items should be clear and actionable, with a specific outcome in mind. This clarity helps in follow-up and accountability.\n",
    "   - Example:\n",
    "     ```\n",
    "     Action Items:\n",
    "     - John Smith to prepare a detailed report on potential overspending areas by April 15, 2024.\n",
    "     - Finance Department to review and propose reallocation strategies by April 30, 2024.\n",
    "     ```\n",
    "\n",
    "6. **Repeat for Each Agenda Item**:\n",
    "   - Repeat steps 2 through 5 for each agenda item discussed during the meeting. Ensure each item is clearly separated and labeled for easy navigation through the document.\n",
    "\n",
    "7. **Formatting Tips**:\n",
    "   - Use headings and subheadings to organize the section and each agenda item. Consider using bold or italicized text to differentiate between titles, names, and the body text.\n",
    "   - Bullet points are useful for summarizing discussion points and listing action items, as they enhance readability.\n",
    "   - Maintain a consistent structure for each agenda item to help readers quickly find information.\n",
    "\n",
    "By following these detailed instructions, you’ll be able to accurately document the Agenda Items section of your meeting minutes. This section is vital for capturing the essence of what was discussed and ensuring that all participants and stakeholders are aware of the discussions and decisions.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "section_3 = \\\n",
    "\"\"\"\n",
    "Instructions for Writing Section 3: Decisions Made with Detailed Information\n",
    "\n",
    "1. **Section Header**:\n",
    "   - Begin this section with a bold header titled \"Decisions Made\" to clearly indicate that this part of the document will cover the conclusive outcomes of the discussions.\n",
    "\n",
    "2. **Listing Decisions**:\n",
    "   - For each decision made during the meeting, create a new sub-section. You can number these decisions for ease of reference. Each decision should have a brief, descriptive title that encapsulates the outcome.\n",
    "\n",
    "3. **Decision Title**:\n",
    "   - Start with \"Decision Title:\" followed by a succinct title that captures the essence of the decision. This helps readers quickly identify the decision’s subject matter.\n",
    "   - Example:\n",
    "     ```\n",
    "     Decision Title: Approval of Q2 Budget Reallocation\n",
    "     ```\n",
    "\n",
    "4. **Description of the Decision**:\n",
    "   - Under the title, provide a detailed description of the decision. Begin with \"Description:\" and elaborate on what was decided, including any specifics that give clarity to the decision's intent, scope, and impact.\n",
    "   - Be precise in detailing the decision to ensure there's no ambiguity about what was agreed upon.\n",
    "   - Example:\n",
    "     ```\n",
    "     Description: The committee unanimously approved the reallocation of funds from the marketing budget to the research and development budget for Q2, increasing R&D funding by 15%.\n",
    "     ```\n",
    "\n",
    "5. **Responsible Party**:\n",
    "   - Identify who is responsible for implementing the decision with \"Responsible Party:\". List the person or department tasked with carrying out the decision, providing clear accountability.\n",
    "   - Example:\n",
    "     ```\n",
    "     Responsible Party: John Doe, Director of Finance\n",
    "     ```\n",
    "\n",
    "6. **Deadline**:\n",
    "   - If applicable, specify a deadline for when the decision needs to be implemented or when a follow-up is required. Use \"Deadline:\" followed by the specific date or timeframe.\n",
    "   - This ensures that there is a clear timeline for action and review.\n",
    "   - Example:\n",
    "     ```\n",
    "     Deadline: Implementation by April 30, 2024\n",
    "     ```\n",
    "\n",
    "7. **Repeat for Each Decision**:\n",
    "   - Repeat steps 2 through 6 for each decision that was made during the meeting. Ensure that each decision is clearly delineated and detailed for easy understanding and reference.\n",
    "\n",
    "8. **Formatting Tips**:\n",
    "   - Use consistent formatting for each decision to help the reader navigate through the section easily. Consistent headers for decision title, description, responsible party, and deadline aid in readability.\n",
    "   - Keep the language clear and direct to avoid any confusion about what was decided.\n",
    "   - Bullet points or numbered lists can be effective for separating different aspects of the decision (e.g., description, responsible party, deadline).\n",
    "\n",
    "By meticulously following these instructions, you'll create a comprehensive and clear \"Decisions Made\" section. This part of the meeting minutes is crucial for documenting the outcomes of discussions and ensuring that all attendees and relevant stakeholders are aware of the actions to be taken.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "section_4 = \\\n",
    "\"\"\"\n",
    "Instructions for Writing Section 4: Action Items with Detailed Information\n",
    "\n",
    "1. **Section Header**:\n",
    "   - Start with a clear header titled \"Action Items\" to indicate this section will enumerate specific tasks to be completed as a result of the meeting's discussions and decisions.\n",
    "\n",
    "2. **Itemizing Action Items**:\n",
    "   - List each action item that was identified during the meeting. You can use a bullet list or a numbered list for clarity. Each action item should be concise yet descriptive enough to convey the task fully.\n",
    "\n",
    "3. **Action Description**:\n",
    "   - For each action item, begin with \"Action:\" followed by a detailed description of the task. This description should be clear and specific, outlining what needs to be done. Avoid vague language to ensure that the action can be executed without additional clarification.\n",
    "   - Example:\n",
    "     ```\n",
    "     Action: Prepare a detailed report comparing Q2 budget projections against actual spending, highlighting areas of concern and potential savings.\n",
    "     ```\n",
    "\n",
    "4. **Assigned To**:\n",
    "   - Specify who is responsible for completing the action item with \"Assigned To:\". This should be an individual's name or, if applicable, a department/team name. Assigning responsibility is crucial for accountability and follow-up.\n",
    "   - Example:\n",
    "     ```\n",
    "     Assigned To: Jane Doe, Budget Analyst\n",
    "     ```\n",
    "\n",
    "5. **Deadline**:\n",
    "   - Set a clear deadline for the action item with \"Deadline:\". Provide a specific date to ensure there is a timeframe for completion. Deadlines help in prioritizing tasks and monitoring progress.\n",
    "   - Example:\n",
    "     ```\n",
    "     Deadline: May 15, 2024\n",
    "     ```\n",
    "\n",
    "6. **Status (Optional)**:\n",
    "   - You may choose to include a \"Status:\" field to note the current progress of the action item at the time of writing the minutes. This can be particularly useful for ongoing tasks or for the next meeting’s follow-up.\n",
    "   - Example:\n",
    "     ```\n",
    "     Status: Assigned\n",
    "     ```\n",
    "\n",
    "7. **Repeat for Each Action Item**:\n",
    "   - Follow steps 2 through 6 for each action item that arises from the meeting’s agenda. Ensure that each action is clearly defined with a responsible party and a deadline for a structured follow-up process.\n",
    "\n",
    "8. **Formatting Tips**:\n",
    "   - Consistent use of bold or italics for key terms (e.g., Action, Assigned To, Deadline) helps distinguish the essential elements of each action item.\n",
    "   - Consider using tables or a structured layout to organize the action items, especially if there are many. This can improve readability and make the document easier to scan.\n",
    "   - Maintain a concise, action-oriented language to ensure each task is clearly understood and actionable.\n",
    "\n",
    "By adhering to these detailed instructions, you will create a comprehensive \"Action Items\" section in your meeting minutes. This part is pivotal for tracking the progress of tasks, ensuring accountability, and facilitating the execution of decisions made during the meeting.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "section_5 = \\\n",
    "\"\"\"\n",
    "Instructions for Writing Section 5: Closing Summary with Detailed Information\n",
    "\n",
    "1. **Section Header**:\n",
    "   - Begin with a bold header titled \"Closing Summary\" to clearly delineate this concluding section from the rest of the document.\n",
    "\n",
    "2. **Meeting Adjournment**:\n",
    "   - Start by noting the time the meeting officially ended with \"Meeting Adjourned:\". This helps to document the meeting's duration and marks the formal conclusion of the session.\n",
    "   - Example:\n",
    "     ```\n",
    "     Meeting Adjourned: 12:00 PM\n",
    "     ```\n",
    "\n",
    "3. **Summary of Decisions and Action Items**:\n",
    "   - Provide a brief overview of the key decisions made and the action items assigned during the meeting. This recap is crucial for reinforcing the outcomes and ensuring that all participants are aligned on the next steps.\n",
    "   - Keep this summary concise and focused on the outcomes that have a significant impact or require immediate attention.\n",
    "   - Example:\n",
    "     ```\n",
    "     Summary of Decisions and Action Items:\n",
    "     - Approved Q2 budget reallocation, increasing R&D funding by 15%.\n",
    "     - Assigned John Doe to prepare a report on budget projections vs. actual spending by May 15, 2024.\n",
    "     ```\n",
    "\n",
    "4. **Next Meeting**:\n",
    "   - If the date, time, and location of the next meeting have already been determined, include this information here. This helps in ensuring that participants can schedule accordingly and prepares them for the next session.\n",
    "   - Example:\n",
    "     ```\n",
    "     Next Meeting: June 10, 2024, at 10:00 AM - Virtual via Zoom\n",
    "     ```\n",
    "\n",
    "5. **Closing Remarks**:\n",
    "   - Conclude with any final remarks or comments from the chairperson. This might include a thank you to the participants, a brief reflection on the meeting's productivity, or encouragement towards the execution of the decided actions.\n",
    "   - Example:\n",
    "     ```\n",
    "     Closing Remarks: The chairperson thanked all participants for their constructive discussions and emphasized the importance of the agreed-upon actions in achieving the department's objectives. All members were encouraged to prioritize the completion of their assigned tasks before the next meeting.\n",
    "     ```\n",
    "\n",
    "6. **Formatting Tips**:\n",
    "   - Use clear, concise language to ensure the closing summary is easily digestible and emphasizes the key takeaways.\n",
    "   - Consider bullet points for the summary of decisions and action items to enhance readability.\n",
    "   - Maintain consistent formatting with the rest of the document, using similar fonts, headings, and layout styles.\n",
    "\n",
    "By meticulously following these instructions, you'll create an effective Closing Summary for your meeting minutes. This section not only provides a succinct recap of the meeting's outcomes but also sets the stage for ongoing collaboration and accountability among participants.\n",
    "\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:49:06.492951Z",
     "start_time": "2024-03-13T07:49:06.486412Z"
    }
   },
   "id": "7f2310369cc7c223",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment Setup\n",
    "\n",
    "## Audio Corpus\n",
    "We will be using an English excerpt from the IMDA National Speech Corpus (NSC). The aim of this transcription, and subsequent minutes generation, is to test the latest `large-v3` Whisper model on the __Singlish__ Accent.\n",
    "\n",
    "We will be testing it on the sample ID `3030` from the `NSC` dataset. This sample contains a conversation between a Singaporean Male and Singaporean Female. The exact lexicon used in the conversation recording includes Singlish phrases i.e. 'aiya', 'leh', 'lah'. We will be testing the model's ability to transcribe these phrases accurately. NSC provides:\n",
    "- Speaker 1's Audio\n",
    "- Speaker 2's Audio\n",
    "- Overall Audio\n",
    "\n",
    "## Transcription Corpus\n",
    "The NSC also provides a transcription of the audio corpus. We will be using this to compare the model's transcription accuracy. NSC provides:\n",
    "- Speaker 1's Transcription\n",
    "- Speaker 2's Transcription\n",
    "\n",
    "This will greatly aid our efforts when comparing the efficacy of the text-level speaker diarization later on. Do note that the transcription is given using the TextGrid format. From my initial analysis, it seems to conform with some variation of SSML. \n",
    "\n",
    "## Model\n",
    "We will be using the latest `large-v3` model offered by OpenAI, running it on the CPU (due to lack of CUDA GPU on my current system), and analysing the transcription accuracy. If we deem that it is up to standard, then we can proceed to __Speaker Diarization__. Else, we may put plans in place to retrain the model on the Singlish Accent. As mentioned above, we are currently using the IMDA NSC as our audio corpus, and it's roughly 890Gb of data. We will be using a small subset of this data for the initial testing.\n",
    "\n",
    "## Future Plans\n",
    "If the model is up to standard, we will proceed to __Speaker Diarization__ and __Minutes Generation__."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a1bb3e06e8034ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the model and the processor\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "model = whisper.load_model(\"large-v3\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:49:20.688433Z",
     "start_time": "2024-03-13T07:49:06.494368Z"
    }
   },
   "id": "506b5f7228044dad",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Meeting-Specific Prompts and Phrases"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31476ebaa0ecca6a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "general = ['Singaporean Singlish Government Business Meeting Transcription Recording']\n",
    "\n",
    "\n",
    "singlish_phrases = [\n",
    "    \"ah\",\"lah\",\"aiya\",\"leh\",\"aiyo\", \"can or not\", \"on the ball\", \"makan session\", \"pow-wow\",\n",
    "    \"kiasu\", \"bo jio\", \"sian\", \"shiok\", \"jialat\",\n",
    "    \"talk cock\", \"wayang\", \"kena\", \"chop-chop\", \"steady\",\n",
    "    \"own time own target (OTOT)\", \"kopi talk\", \"catch up\", \"brainstorm\", \"align\",\n",
    "    \"lobang\", \"paiseh\", \"action\", \"agaration\", \"angkat bola\",\n",
    "    \"bao ga liao\", \"buay pai\", \"cheem\", \"chio\", \"garang\",\n",
    "    \"goondu\", \"kaypoh\", \"leh\", \"lor\", \"nia\",\n",
    "    \"one corner\", \"open table\", \"pai seh\", \"relak one corner\", \"sabo\",\n",
    "    \"sai kang\", \"shiok\", \"siam\", \"sikit-sikit\", \"suay\",\n",
    "    \"tabao\", \"talk shop\", \"tan tio\", \"up lorry\", \"wa kau\"\n",
    "]\n",
    "\n",
    "singlish_business_phrases = [\n",
    "    \"lah\", \"can or not?\", \"on the ball\", \"kiasu\", \"shiok\",\n",
    "    \"talk cock\", \"steady pom pi pi\", \"own time own target\", \"bo jio\", \"catch no ball\",\n",
    "    \"chiong\", \"chop chop\", \"die die must do\", \"eat snake\", \"gostan\",\n",
    "    \"jialat\", \"kaypoh\", \"leh\", \"lor\", \"makan\",\n",
    "    \"nabei\", \"paiseh\", \"sabo\", \"sian\", \"suay\",\n",
    "    \"walao eh\", \"wayang\", \"win already lor\", \"yaya papaya\", \"zi high\",\n",
    "    \"send it\", \"check back next week\", \"let’s touch base on this\", \"circle back on that\", \"park this for now\",\n",
    "    \"align our ducks\", \"low key\", \"see how\", \"can make it\", \"noted with thanks\",\n",
    "    \"bo bian\", \"anyhow\", \"confirm plus chop\", \"got chance\", \"mai tu liao\",\n",
    "    \"double confirm\", \"one shot\", \"over already\", \"swee\", \"talk later\"\n",
    "]\n",
    "\n",
    "singlish_more_formal_phrases= [\n",
    "    'appreciate your feedback', 'before we proceed', \"let's take this offline\", 'moving forward', 'on the same page',\n",
    "    'per your suggestion', 'please advise', 'point of contact', 'scope of work', 'stakeholder engagement',\n",
    "    'strategic priorities', 'target milestones', 'thank you for your patience', 'timeline for completion', 'touch base next week',\n",
    "    \"we'll circle back on this\", 'action items', 'align our strategies', 'benchmark for success', 'best practices in the industry',\n",
    "    'client satisfaction', 'competitive advantage', 'comprehensive review', 'cost-effective solutions', 'cross-functional collaboration',\n",
    "    'due diligence', 'enhance our capabilities', 'feedback loop', 'forward-thinking approach', 'holistic strategy',\n",
    "    'implementation phase', 'key takeaways', 'leverage our strengths', 'maximize efficiency', 'ongoing support',\n",
    "    'optimize performance', 'proactive measures', 'quality assurance', 'risk management strategies', 'seamless integration',\n",
    "    'stakeholder feedback', 'sustainable growth', 'tailored solutions', 'value proposition', 'win-win situation',\n",
    "    'workflow optimization', 'zero in on the details', 'drive innovation', 'escalate this issue', 'monitor progress'\n",
    "]\n",
    "# common_words = [\n",
    "#     'agenda', 'align', 'benchmark', 'best practice', 'bottom line',\n",
    "#     'brainstorm', 'brand', 'budget', 'buy-in', 'capacity',\n",
    "#     'capital', 'collaborate', 'competitive', 'compliance', 'deliverable',\n",
    "#     'disruptive', 'diversify', 'efficiency', 'engagement', 'execution',\n",
    "#     'forecast', 'growth', 'innovate', 'insight', 'investment',\n",
    "#     'KPI (Key Performance Indicator)', 'leverage', 'metrics', 'milestone', 'networking',\n",
    "#     'objective', 'optimize', 'outcome', 'outsourcing', 'performance',\n",
    "#     'prioritize', 'profitability', 'project', 'ROI (Return on Investment)', 'scalability',\n",
    "#     'stakeholder', 'strategy', 'synergy', 'target', 'timeline',\n",
    "#     'traction', 'value', 'vision', 'workflow', 'yield'\n",
    "# ]\n",
    "# additional_business_words = [\n",
    "#     \"touch base\", \"heads up\", \"debrief\", \"downtime\", \"feedback\", \n",
    "#     \"game plan\", \"goal\", \"hangout\", \"initiative\", \"kickoff\", \n",
    "#     \"loop in\", \"milestones\", \"network\", \"nitty-gritty\", \"onboard\", \n",
    "#     \"ping\", \"pivot\", \"proactive\", \"ramp up\", \"reach out\", \n",
    "#     \"recap\", \"roadmap\", \"run-through\", \"scope\", \"sidebar\", \n",
    "#     \"silo\", \"sprint\", \"stakeholders\", \"stand-up\", \"startup\", \n",
    "#     \"strategy session\", \"streamline\", \"touchpoint\", \"track\", \"upskill\", \n",
    "#     \"value-add\", \"win-win\", \"workaround\", \"workshop\", \"zoom in\", \n",
    "#     \"bandwidth\", \"deep dive\", \"ecosystem\", \"empower\", \"granular\",\n",
    "#     \"holistic\", \"ideate\", \"iterate\", \"low-hanging fruit\", \"paradigm shift\"\n",
    "# ]\n",
    "collated_list = general  + singlish_phrases + singlish_business_phrases + singlish_more_formal_phrases # +common_words + additional_business_words\n",
    "\n",
    "collated_list_string = ' '.join(collated_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:49:20.714831Z",
     "start_time": "2024-03-13T07:49:20.696327Z"
    }
   },
   "id": "4a5f758dc67739c9",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performing Transcription"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "653a11d8810f95ef"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00.000 --> 00:06.000]  or the carnival, want to get you up. Especially when you look like a piggy.\n",
      "[00:06.000 --> 00:07.000]  Who said so?\n",
      "[00:07.000 --> 00:09.000]  Said, oh, very nice man.\n",
      "[00:09.000 --> 00:11.000]  Oh, so did you drive there?\n",
      "[00:11.000 --> 00:13.000]  Huh? No, I walk over there.\n",
      "[00:13.000 --> 00:14.000]  You? Huh?\n",
      "[00:14.000 --> 00:15.000]  I walk over there.\n",
      "[00:15.000 --> 00:17.000]  How do you walk?\n",
      "[00:17.000 --> 00:19.000]  I can walk, no problem at all.\n",
      "[00:19.000 --> 00:21.000]  How many kilometers you walk?\n",
      "[00:21.000 --> 00:23.000]  I can't remember.\n",
      "[00:23.000 --> 00:24.000]  Are you?\n",
      "[00:24.000 --> 00:26.000]  Because I walk until my legs look thin.\n",
      "[00:26.000 --> 00:29.000]  So will you go there? Will you go back again?\n",
      "[00:29.000 --> 00:31.000]  I don't think so.\n",
      "[00:31.000 --> 00:32.000]  Why?\n",
      "[00:32.000 --> 00:35.000]  Because it's too much. It's too much to remember.\n",
      "[00:35.000 --> 00:38.000]  So which is your next destination?\n",
      "[00:38.000 --> 00:42.000]  Maybe next one go to another part of Africa.\n",
      "[00:42.000 --> 00:45.000]  Western Africa. No, it's more challenging.\n",
      "[00:45.000 --> 00:46.000]  Oh, is it?\n",
      "[00:46.000 --> 00:49.000]  Because it's more fun than what you expect to be.\n",
      "[00:49.000 --> 00:54.000]  Then why don't you go to South Australia?\n",
      "[00:54.000 --> 00:56.000]  South Australia.\n",
      "[00:56.000 --> 00:59.000]  South Australia is nothing compared to Western Australia.\n",
      "[00:59.000 --> 01:01.000]  Because it's too much Africa.\n",
      "[01:01.000 --> 01:04.000]  Because Africa is a very wild place to go.\n",
      "[01:04.000 --> 01:09.000]  Yeah, I mean, until you go there, you say, I don't want to go again.\n",
      "[01:09.000 --> 01:12.000]  Because Australia, you go, you want to go again and again and again.\n",
      "[01:12.000 --> 01:16.000]  But why do so many people want to catch the Northern Lights?\n",
      "[01:16.000 --> 01:20.000]  Because there's no natural light that looks like Northern Lights.\n",
      "[01:20.000 --> 01:23.000]  That's why they want to catch the Northern Lights.\n",
      "[01:23.000 --> 01:26.000]  Because they thought it's a special fluorescent light.\n",
      "[01:26.000 --> 01:28.000]  But you can make the same effect in your computer.\n",
      "[01:28.000 --> 01:31.000]  But I thought the lights can dance.\n",
      "[01:31.000 --> 01:32.000]  Huh?\n",
      "[01:32.000 --> 01:36.000]  I thought some got frostbite when they have...\n",
      "[01:36.000 --> 01:39.000]  Because they stay out in the cold for too long. That's why.\n",
      "[01:39.000 --> 01:42.000]  Until the light makes them freeze.\n",
      "[01:42.000 --> 01:46.000]  That's why it's called make them freeze. That's why it makes them frozen like that.\n",
      "[01:46.000 --> 01:48.000]  Do you feel like camping over there?\n",
      "[01:48.000 --> 01:49.000]  Not really.\n",
      "[01:49.000 --> 01:50.000]  Why?\n",
      "[01:50.000 --> 01:52.000]  You camp there, you can do what? What can you do?\n",
      "[01:52.000 --> 01:56.000]  You want to have the light, easy lah. Just buy a light.\n",
      "[01:56.000 --> 01:58.000]  Then you make it like during time.\n",
      "[01:58.000 --> 02:00.000]  Then you make it like a color. Then it becomes like Northern Lights.\n",
      "[02:00.000 --> 02:02.000]  Then how about the dancing effect?\n",
      "[02:02.000 --> 02:06.000]  Dancing effect, you make the lamp move around lah. Then it becomes moving dancing lights.\n",
      "[02:06.000 --> 02:08.000]  Same effect as Northern Lights.\n",
      "[02:08.000 --> 02:09.000]  That's so creative.\n",
      "[02:09.000 --> 02:10.000]  Yeah.\n",
      "[02:10.000 --> 02:14.000]  That's very interesting. That's the man-made Northern Lights.\n",
      "[02:14.000 --> 02:20.000]  So you don't need to worry about paying all the air ticket, accommodation, and all your transfer to see the Northern Lights.\n",
      "[02:20.000 --> 02:21.000]  That's a great idea.\n",
      "[02:21.000 --> 02:23.000]  And you won't get frostbite for sure.\n",
      "[02:23.000 --> 02:24.000]  100%.\n",
      "[02:24.000 --> 02:26.000]  Then how about the husky dogs?\n",
      "[02:26.000 --> 02:28.000]  Husky dog you can go to Thailand no problem.\n",
      "[02:28.000 --> 02:31.000]  I want to go to the husky dog cafe.\n",
      "[02:31.000 --> 02:33.000]  You can go yourself, thank you.\n",
      "[02:33.000 --> 02:39.000]  Please. I want to go to the raccoon cafe in Seoul, South Korea.\n",
      "[02:39.000 --> 02:41.000]  Raccoon lah. That one, the raccoon also in.\n",
      "[02:41.000 --> 02:46.000]  That's a creature in the Guardians of the Galaxy.\n",
      "[02:46.000 --> 02:50.000]  Then you go and find Guardians of the Galaxy lah. That one is the raccoon. That one is the talking raccoon.\n",
      "[02:50.000 --> 02:52.000]  Yeah, I like the talking raccoon.\n",
      "[02:52.000 --> 02:55.000]  The soap token, the raccoon cafe cannot talk one lah.\n",
      "[02:55.000 --> 03:00.000]  Who say so? You bring me there, then maybe the raccoon can talk.\n",
      "[03:00.000 --> 03:02.000]  I don't think so lah.\n",
      "[03:02.000 --> 03:05.000]  You might say you look like a stupid raccoon. Then they will just call you back.\n",
      "[03:05.000 --> 03:07.000]  Then there is the talking raccoon.\n",
      "[03:07.000 --> 03:09.000]  Then you, what are you talking about?\n",
      "[03:12.000 --> 03:14.000]  Then you become the...\n",
      "[03:14.000 --> 03:16.000]  The D-star, what star?\n",
      "[03:16.000 --> 03:17.000]  What's the name?\n",
      "[03:17.000 --> 03:20.000]  Forget it. The Cadet of the Galaxy.\n",
      "[03:20.000 --> 03:21.000]  Oh yeah?\n",
      "[03:21.000 --> 03:22.000]  Yeah.\n",
      "[03:22.000 --> 03:23.000]  Wow.\n",
      "[03:23.000 --> 03:24.000]  Baron.\n",
      "[03:24.000 --> 03:25.000]  Oh.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# calling this will actually result in the model.transcribe returning the transcribed text + timestamps into the cell output itself.\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranscribe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./content/singlish_accent/3030_trimmed/3030-combined_trimmed.wav\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollated_list_string\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/transcribe.py:240\u001B[0m, in \u001B[0;36mtranscribe\u001B[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001B[0m\n\u001B[1;32m    237\u001B[0m mel_segment \u001B[38;5;241m=\u001B[39m pad_or_trim(mel_segment, N_FRAMES)\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\u001B[38;5;241m.\u001B[39mto(dtype)\n\u001B[1;32m    239\u001B[0m decode_options[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m all_tokens[prompt_reset_since:]\n\u001B[0;32m--> 240\u001B[0m result: DecodingResult \u001B[38;5;241m=\u001B[39m \u001B[43mdecode_with_fallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmel_segment\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    241\u001B[0m tokens \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(result\u001B[38;5;241m.\u001B[39mtokens)\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m no_speech_threshold \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;66;03m# no voice activity check\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/transcribe.py:170\u001B[0m, in \u001B[0;36mtranscribe.<locals>.decode_with_fallback\u001B[0;34m(segment)\u001B[0m\n\u001B[1;32m    167\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_of\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    169\u001B[0m options \u001B[38;5;241m=\u001B[39m DecodingOptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs, temperature\u001B[38;5;241m=\u001B[39mt)\n\u001B[0;32m--> 170\u001B[0m decode_result \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43msegment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    172\u001B[0m needs_fallback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    174\u001B[0m     compression_ratio_threshold \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m decode_result\u001B[38;5;241m.\u001B[39mcompression_ratio \u001B[38;5;241m>\u001B[39m compression_ratio_threshold\n\u001B[1;32m    176\u001B[0m ):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/decoding.py:824\u001B[0m, in \u001B[0;36mdecode\u001B[0;34m(model, mel, options, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs:\n\u001B[1;32m    822\u001B[0m     options \u001B[38;5;241m=\u001B[39m replace(options, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 824\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mDecodingTask\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m single \u001B[38;5;28;01melse\u001B[39;00m result\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/decoding.py:737\u001B[0m, in \u001B[0;36mDecodingTask.run\u001B[0;34m(self, mel)\u001B[0m\n\u001B[1;32m    734\u001B[0m tokens \u001B[38;5;241m=\u001B[39m tokens\u001B[38;5;241m.\u001B[39mrepeat_interleave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_group, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(audio_features\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    736\u001B[0m \u001B[38;5;66;03m# call the main sampling loop\u001B[39;00m\n\u001B[0;32m--> 737\u001B[0m tokens, sum_logprobs, no_speech_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_main_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    739\u001B[0m \u001B[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001B[39;00m\n\u001B[1;32m    740\u001B[0m audio_features \u001B[38;5;241m=\u001B[39m audio_features[:: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_group]\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/decoding.py:687\u001B[0m, in \u001B[0;36mDecodingTask._main_loop\u001B[0;34m(self, audio_features, tokens)\u001B[0m\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    686\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample_len):\n\u001B[0;32m--> 687\u001B[0m         logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minference\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    689\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    690\u001B[0m             i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mno_speech \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    691\u001B[0m         ):  \u001B[38;5;66;03m# save no_speech_probs\u001B[39;00m\n\u001B[1;32m    692\u001B[0m             probs_at_sot \u001B[38;5;241m=\u001B[39m logits[:, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msot_index]\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39msoftmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/decoding.py:163\u001B[0m, in \u001B[0;36mPyTorchInference.logits\u001B[0;34m(self, tokens, audio_features)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokens\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitial_token_length:\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;66;03m# only need to use the last token except in the first forward pass\u001B[39;00m\n\u001B[1;32m    161\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m tokens[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m--> 163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkv_cache\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/model.py:211\u001B[0m, in \u001B[0;36mTextDecoder.forward\u001B[0;34m(self, x, xa, kv_cache)\u001B[0m\n\u001B[1;32m    208\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(xa\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m--> 211\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_cache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    213\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln(x)\n\u001B[1;32m    214\u001B[0m logits \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    215\u001B[0m     x \u001B[38;5;241m@\u001B[39m torch\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_embedding\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdtype), \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    216\u001B[0m )\u001B[38;5;241m.\u001B[39mfloat()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/model.py:138\u001B[0m, in \u001B[0;36mResidualAttentionBlock.forward\u001B[0;34m(self, x, xa, mask, kv_cache)\u001B[0m\n\u001B[1;32m    136\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_ln(x), mask\u001B[38;5;241m=\u001B[39mmask, kv_cache\u001B[38;5;241m=\u001B[39mkv_cache)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcross_attn:\n\u001B[0;32m--> 138\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_attn_ln\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_cache\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    139\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_ln(x))\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/model.py:90\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[0;34m(self, x, xa, mask, kv_cache)\u001B[0m\n\u001B[1;32m     87\u001B[0m     k \u001B[38;5;241m=\u001B[39m kv_cache[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey]\n\u001B[1;32m     88\u001B[0m     v \u001B[38;5;241m=\u001B[39m kv_cache[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue]\n\u001B[0;32m---> 90\u001B[0m wv, qk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mqkv_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout(wv), qk\n",
      "File \u001B[0;32m/opt/miniconda3/envs/minutes_gpt/lib/python3.10/site-packages/whisper/model.py:108\u001B[0m, in \u001B[0;36mMultiHeadAttention.qkv_attention\u001B[0;34m(self, q, k, v, mask)\u001B[0m\n\u001B[1;32m    105\u001B[0m qk \u001B[38;5;241m=\u001B[39m qk\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m    107\u001B[0m w \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(qk, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(q\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m--> 108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten(start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m), qk\u001B[38;5;241m.\u001B[39mdetach()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# calling this will actually result in the model.transcribe returning the transcribed text + timestamps into the cell output itself.\n",
    "result = model.transcribe(\"./content/singlish_accent/3030_trimmed/3030-combined_trimmed.wav\", verbose=True, language=\"en\", prompt=collated_list_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:53:11.789917Z",
     "start_time": "2024-03-13T07:49:20.716553Z"
    }
   },
   "id": "fde3c9b1e5da7ddc",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transcription Cleanup and Diarization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "645ea9f2abc2e712"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m per_line \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m segment \u001B[38;5;129;01min\u001B[39;00m \u001B[43mresult\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msegments\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m      3\u001B[0m     text_to_append \u001B[38;5;241m=\u001B[39m segment[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      4\u001B[0m     text_to_append \u001B[38;5;241m=\u001B[39m text_to_append[\u001B[38;5;241m1\u001B[39m:]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "per_line = []\n",
    "for segment in result['segments']:\n",
    "    text_to_append = segment['text']\n",
    "    text_to_append = text_to_append[1:]\n",
    "    per_line.append(text_to_append)\n",
    "\n",
    "per_line"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:53:15.840958Z",
     "start_time": "2024-03-13T07:53:15.829324Z"
    }
   },
   "id": "9e054cef7289f09c",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Diarization\n",
    "\n",
    "For this, we input the whole transcript into the model, but we run the risk of running out of context."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b01a1b29837ce739"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# diarization = client.chat.completions.create(\n",
    "# \t\t\t\t\tmodel=DEPLOYMENT,\n",
    "# \t\t\t\t\tmessages=[\n",
    "# \t\t\t\t\t\t{\"role\": \"system\", \"content\": \"You are a linguistics expert with 50 years of experience. You will be given a list of sentence, and you are to assign the Speaker label to each sentence PER line. I.e. Given ['It wasn't my fault', 'I didn't say it was', 'Don't accuse me'], you will return me: ['Speaker 1', 'Speaker 2', 'Speaker 2']\"},\n",
    "# \t\t\t\t\t\t{\"role\": \"user\", \"content\": f\"Here is the list of sentences: {per_line}. You will diarize ALL the words/phrases in the list. JUST RETURN ME THE LIST. You WILL ensure that you labeled EVERY element.\"}\n",
    "# \t\t\t\t\t],\n",
    "# \t\t\t\t\tmax_tokens=3000,\n",
    "# \t\t\t\t\tstream=False,\n",
    "# \t\t\t\t\ttemperature=0.5,\n",
    "# \t\t\t\t)\n",
    "# end_result = diarization.choices[0].message.content\n",
    "# end_result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53e5aa18eb69a9f8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the end result is broken and unreliable. As such, we will need to perform a more reliable diarization method.\n",
    "\n",
    "We will proceed to a Chunk+Stride diarization method."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae7774696b6ac4d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chunk+Stride Diarization\n",
    "\n",
    "For the Chunk+Stride Diarization method, we will chunk the list into `x` number of chunks, and for each chunk, we will also add a stride of `a` behind and in front of the chunk. This will allow us to capture the context of the conversation better, and thus, provide a more accurate diarization.\n",
    "\n",
    "\\begin{equation}\n",
    "N = \\text{length of list}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "c = \\text{chunk size}, \\, a = \\text{stride length}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "x = \\left\\lceil \\frac{N}{c} \\right\\rceil, \\, \\text{number of chunks}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{Initial chunks definition:}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "S_i = i \\cdot c, \\, E_i = \\min((i+1) \\cdot c - 1, N-1), \\, 0 \\leq i < x\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{Stride modifications:}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{For } i = 1 \\text{ to } x-2:\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{Prepend } \\text{sort}(\\text{last } a \\text{ elements of } \\text{chunk}_{i-1}, \\text{desc}) \\text{ to } \\text{chunk}_i\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{Append } \\text{sort}(\\text{first } a \\text{ elements of } \\text{chunk}_{i+1}) \\text{ to } \\text{chunk}_i\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\text{For } i = 0 \\text{ and } i = x-1, \\text{ chunks remain unchanged.}\n",
    "\\end{equation}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3565ced60af225dc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21]]\n",
      "[[0, 1, 2, 3, 4, 5], [4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [16, 17, 18, 19, 20, 21]]\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def chunk_with_stride(initial_list:list, stride:int, number_of_chunks:int):\n",
    "    N = len(initial_list)\n",
    "    # create initial chunks\n",
    "    initial_chunks = [initial_list[i * number_of_chunks:(i + 1) * number_of_chunks] for i in range((N + number_of_chunks - 1) // number_of_chunks )] \n",
    "    stride_chunks = []\n",
    "    # print(len(initial_chunks))\n",
    "    print(initial_chunks)\n",
    "    for chunk_index in range(len(initial_chunks)):\n",
    "        if chunk_index == 0:\n",
    "            stride_chunks.append(initial_chunks[chunk_index])\n",
    "            \n",
    "        elif 0 < chunk_index < len(initial_chunks)-1:\n",
    "            current_chunk = deque(initial_chunks[chunk_index])\n",
    "            # retrieve `a` number of elements from the previous chunk to perform backwards stride\n",
    "            previous_chunk_elements = initial_chunks[chunk_index-1]\n",
    "            previous_stride_elements = previous_chunk_elements[-stride:]\n",
    "            previous_stride_elements.sort(reverse=True)\n",
    "            for past_element in previous_stride_elements:\n",
    "                current_chunk.appendleft(past_element)\n",
    "                \n",
    "            future_chunk_elements = initial_chunks[chunk_index+1]\n",
    "            future_stride_elements = future_chunk_elements[:stride]\n",
    "            future_stride_elements.sort()\n",
    "            for future_element in future_stride_elements:\n",
    "                current_chunk.append(future_element)\n",
    "\n",
    "            stride_chunks.append(list(current_chunk))\n",
    "            # print('current strided chunk:', current_chunk)\n",
    "            \n",
    "        elif chunk_index == len(initial_chunks)-1:\n",
    "            current_chunk = deque(initial_chunks[chunk_index])\n",
    "            # retrieve `a` number of elements from the previous chunk to perform backwards stride\n",
    "            previous_chunk_elements = initial_chunks[chunk_index-1]\n",
    "            previous_stride_elements = previous_chunk_elements[-stride:]\n",
    "            previous_stride_elements.sort(reverse=True)\n",
    "            for past_element in previous_stride_elements:\n",
    "                current_chunk.appendleft(past_element)\n",
    "                \n",
    "            stride_chunks.append(list(current_chunk))\n",
    "            # print('current strided chunk:', current_chunk)\n",
    "            \n",
    "    print(stride_chunks)\n",
    "\n",
    "        \n",
    "original_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "\n",
    "chunk_with_stride(original_list, 2, 6)\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T08:22:18.983809Z",
     "start_time": "2024-03-13T08:22:18.973426Z"
    }
   },
   "id": "20fca5eb9152adca",
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label-Aware Adaptive Diarization\n",
    "\n",
    "With the strategy of updating chunks to include strides (for incorporating past and future context), the next step involves implementing a function to iterate through this modified chunk list for diarization purposes.\n",
    "\n",
    "- **Backstride**: Elements from the past stride\n",
    "- **Forwardstride**: Elements from the future stride\n",
    "\n",
    "Additionally, an enhancement will be integrated where each chunk, upon submission, will carry the respective speaker labels for the preceding `a` elements, enriching the context for improved accuracy.\n",
    "\n",
    "### Example:\n",
    "Consider the initial chunked list and its updated version with a stride `a=2`:\n",
    "\n",
    "- **Initial list**: `[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21]]`\n",
    "- **Updated with stride**: `[[0, 1, 2, 3, 4, 5], [4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [16, 17, 18, 19, 20, 21]]`\n",
    "\n",
    "For diarization, beginning with chunk 1 (`0<=i<x`), specifically the second chunk `[4, 5, 6, 7, 8, 9, 10, 11, 12, 13]`, the last `a=2` speaker labels from the `backstride` will be incorporated into the context. \n",
    "\n",
    "If, for instance, the speaker labels for elements `4` and `5` are `Speaker 1`, this provides the Language Model (LLM) with valuable historical context from the previous diarization session, enabling more precise diarization results.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae4cb0903da149df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
